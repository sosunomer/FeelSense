import streamlit as st
import cv2
import numpy as np
import pickle
import time
from PIL import Image
import io
try:
    import pyttsx3
    SPEECH_AVAILABLE = True
except ImportError:
    SPEECH_AVAILABLE = False
import threading

# Sabit deÄŸiÅŸkenler
EMOTION_LABELS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
FOLDER_LABELS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']
MODEL_PATH = 'emotion_model.pkl'
FACE_CLASSIFIER_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
IMG_HEIGHT, IMG_WIDTH = 48, 48

# Duygu etiketlerinin TÃ¼rkÃ§e karÅŸÄ±lÄ±klarÄ±
EMOTION_LABELS_TR = {
    'Angry': 'KÄ±zgÄ±n',
    'Disgust': 'Ä°ÄŸrenme',
    'Fear': 'Korku',
    'Happy': 'Mutlu',
    'Sad': 'ÃœzgÃ¼n',
    'Surprise': 'ÅaÅŸkÄ±n',
    'Neutral': 'NÃ¶tr'
}

# Ses motoru baÅŸlatma
def initialize_speech_engine():
    if not SPEECH_AVAILABLE:
        return None
    try:
        engine = pyttsx3.init()
        engine.setProperty('rate', 150)  # KonuÅŸma hÄ±zÄ±
        engine.setProperty('volume', 1.0)  # Ses seviyesi
        return engine
    except Exception as e:
        print(f"Ses motoru baÅŸlatÄ±lamadÄ±: {e}")
        return None

# Sesli duygu bildirimi
def speak_emotion(emotion, engine):
    if engine is None:
        return
    
    def speak_thread():
        try:
            # TÃ¼rkÃ§e karÅŸÄ±lÄ±ÄŸÄ±nÄ± bul
            emotion_tr = EMOTION_LABELS_TR.get(emotion, emotion)
            engine.say(f"Tespit edilen duygu: {emotion_tr}")
            engine.runAndWait()
        except Exception as e:
            print(f"Sesli bildirim hatasÄ±: {e}")
    
    # KonuÅŸmayÄ± ayrÄ± bir thread'de Ã§alÄ±ÅŸtÄ±r
    threading.Thread(target=speak_thread).start()

# Streamlit sayfa yapÄ±landÄ±rmasÄ±
st.set_page_config(
    page_title="FeelSense - Emotion Recognition",
    page_icon="ğŸ˜Š",
    layout="wide"
)

# BaÅŸlÄ±k ve aÃ§Ä±klama
st.title("FeelSense - Real-Time Emotion Recognition")
st.markdown("""
This application analyzes facial expressions from camera images to predict emotional states.
""")

# Yan panel ayarlarÄ±
with st.sidebar:
    st.header("Settings")
    detection_confidence = st.slider("Detection Confidence Threshold", 0.0, 1.0, 0.5, 0.05)
    st.divider()
    st.subheader("About")
    st.info("This application is developed using Scikit-learn and OpenCV.")
    st.markdown("Supported emotions:")
    for emotion in EMOTION_LABELS:
        st.markdown(f"- {emotion}")

@st.cache_resource
def load_emotion_model():
    """EÄŸitilmiÅŸ duygu tanÄ±ma modelini yÃ¼kler"""
    try:
        with open(MODEL_PATH, 'rb') as f:
            model = pickle.load(f)
        return model
    except Exception as e:
        st.error(f"Model yÃ¼klenirken hata oluÅŸtu: {e}")
        return None

def process_image(image, face_classifier, emotion_model, speech_engine=None, speak=False):
    """GÃ¶rÃ¼ntÃ¼yÃ¼ iÅŸleyip duygu tanÄ±ma yapar"""
    # GÃ¶rÃ¼ntÃ¼yÃ¼ OpenCV formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
    img_array = np.array(image)
    img = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
    
    # GÃ¶rÃ¼ntÃ¼yÃ¼ gri tonlamaya dÃ¶nÃ¼ÅŸtÃ¼r
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    
    # YÃ¼zleri tespit et
    faces = face_classifier.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
    
    results = []
    
    # Tespit edilen her yÃ¼z iÃ§in duygu tanÄ±ma yap
    for (x, y, w, h) in faces:
        # YÃ¼z bÃ¶lgesini kÄ±rp
        roi_gray = gray[y:y+h, x:x+w]
        
        # Modele uygun boyuta getir ve normalize et
        roi = cv2.resize(roi_gray, (IMG_HEIGHT, IMG_WIDTH), interpolation=cv2.INTER_AREA)
        roi = roi.flatten().astype('float') / 255.0
        
        # Duygu tahmini yap
        try:
            prediction = emotion_model.predict_proba([roi])[0]
            emotion_idx = np.argmax(prediction)
            emotion_label = EMOTION_LABELS[emotion_idx]
            confidence = prediction[emotion_idx]
            
            # SonuÃ§larÄ± gÃ¶rÃ¼ntÃ¼ye ekle
            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)
            
            # Duygu etiketi ve gÃ¼ven deÄŸeri
            label_text = f"{emotion_label}: {confidence:.1f}%"
            cv2.putText(img, label_text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
            
            # SonuÃ§larÄ± listeye ekle
            results.append({
                "emotion": emotion_label,
                "confidence": confidence,
                "position": (x, y, w, h)
            })
            
            # Sesli bildirim
            if speak and speech_engine and confidence > 0.5:  # Sadece gÃ¼ven deÄŸeri yÃ¼ksek olanlarÄ± seslendir
                speak_emotion(emotion_label, speech_engine)
                
        except Exception as e:
            st.error(f"Tahmin hatasÄ±: {e}")
    
    # GÃ¶rÃ¼ntÃ¼yÃ¼ RGB formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r (Streamlit iÃ§in)
    result_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    return result_img, results

def main():
    # YÃ¼z sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±nÄ± yÃ¼kle
    face_classifier = cv2.CascadeClassifier(FACE_CLASSIFIER_PATH)
    
    # Duygu tanÄ±ma modelini yÃ¼kle
    emotion_model = load_emotion_model()
    if emotion_model is None:
        st.error("Model could not be loaded. Please check if the model file is in the correct location.")
        return
    
    # Ses motorunu baÅŸlat
    speech_engine = initialize_speech_engine()
    if speech_engine is None and SPEECH_AVAILABLE:
        st.info("Sesli bildirim Ã¶zelliÄŸi bu platformda kullanÄ±lamÄ±yor.")
    elif not SPEECH_AVAILABLE:
        st.info("Ses kÃ¼tÃ¼phanesi yÃ¼klÃ¼ deÄŸil. Uygulama sessiz modda Ã§alÄ±ÅŸacak.")
    
    # Sekme seÃ§enekleri
    tab1, tab2 = st.tabs(["ğŸ“· Camera", "ğŸ–¼ï¸ Upload Photo"])
    
    with tab1:
        st.header("Emotion Recognition with Camera")
        
        # Kamera kontrolÃ¼ iÃ§in session state kullan
        if 'camera_on' not in st.session_state:
            st.session_state.camera_on = False
        if 'frame_count' not in st.session_state:
            st.session_state.frame_count = 0
        
        # Sesli bildirim seÃ§eneÄŸi
        enable_voice = st.checkbox("Enable voice feedback", value=True)
        
        # Kamera baÅŸlatma/durdurma butonlarÄ±
        col1, col2 = st.columns(2)
        
        with col1:
            if not st.session_state.camera_on:
                start_button = st.button("Start Camera", type="primary", use_container_width=True, key="start_btn")
                if start_button:
                    st.session_state.camera_on = True
                    st.session_state.frame_count = 0
                    st.rerun()
        
        with col2:
            if st.session_state.camera_on:
                stop_button = st.button("Stop Camera", type="secondary", use_container_width=True, key="stop_btn")
                if stop_button:
                    st.session_state.camera_on = False
                    st.rerun()
        
        # Kamera gÃ¶rÃ¼ntÃ¼sÃ¼ iÃ§in yer tutucu
        camera_container = st.container()
        result_container = st.container()
        
        # Kamera aÃ§Ä±ksa gÃ¶rÃ¼ntÃ¼ akÄ±ÅŸÄ±nÄ± baÅŸlat
        if st.session_state.camera_on:
            with camera_container:
                st.info("ğŸ“¹ Camera is active. Click 'Stop Camera' to exit.")
                
                # Placeholder'larÄ± container iÃ§inde oluÅŸtur
                camera_placeholder = st.empty()
                
            with result_container:
                result_placeholder = st.empty()
            
            # Webcam widget kullanÄ±mÄ± (daha stabil)
            try:
                # Basit kamera gÃ¶rÃ¼ntÃ¼sÃ¼ alma
                img_file_buffer = st.camera_input("Take a picture for emotion analysis", key="camera_input")
                
                if img_file_buffer is not None:
                    # GÃ¶rÃ¼ntÃ¼yÃ¼ iÅŸle
                    image = Image.open(img_file_buffer)
                    result_img, results = process_image(
                        image, 
                        face_classifier, 
                        emotion_model,
                        speech_engine if enable_voice else None,
                        enable_voice
                    )
                    
                    # SonuÃ§larÄ± gÃ¶ster
                    with camera_container:
                        st.image(result_img, caption="Analysis Result", width=640)
                    
                    with result_container:
                        if results:
                            st.success("âœ… Faces detected!")
                            for i, result in enumerate(results):
                                st.write(f"Face {i+1}: **{result['emotion']}** ({result['confidence']:.1%} confidence)")
                        else:
                            st.warning("No faces detected in the image.")
                            
            except Exception as e:
                st.error(f"Camera error: {str(e)}")
                st.session_state.camera_on = False
        else:
            with camera_container:
                st.info("ğŸ“¸ Click 'Start Camera' to activate emotion recognition.")
    
    with tab2:
        st.header("Emotion Recognition with Photo")
        
        # Sesli bildirim seÃ§eneÄŸi
        enable_voice_photo = st.checkbox("Enable voice feedback for photo", value=True)
        
        # Dosya yÃ¼kleme
        uploaded_file = st.file_uploader("Upload a photo", type=["jpg", "jpeg", "png"])
        
        if uploaded_file is not None:
            # YÃ¼klenen gÃ¶rÃ¼ntÃ¼yÃ¼ oku
            image = Image.open(uploaded_file)
            
            # GÃ¶rÃ¼ntÃ¼yÃ¼ gÃ¶ster
            st.image(image, caption="Uploaded photo", width=640)
            
            # GÃ¶rÃ¼ntÃ¼yÃ¼ iÅŸle
            if st.button("Analyze Emotions", type="primary"):
                with st.spinner("Analyzing..."):
                    result_img, results = process_image(
                        image, 
                        face_classifier, 
                        emotion_model,
                        speech_engine if enable_voice_photo else None,
                        enable_voice_photo
                    )
                    
                    # Ä°ÅŸlenmiÅŸ gÃ¶rÃ¼ntÃ¼yÃ¼ gÃ¶ster
                    st.image(result_img, caption="Analysis result", width=640)
                    
                    # Duygu sonuÃ§larÄ±nÄ± gÃ¶ster
                    if results:
                        st.subheader("Detected emotions:")
                        for i, result in enumerate(results):
                            st.write(f"Face {i+1}: {result['emotion']} ({result['confidence']:.1%} confidence)")
                    else:
                        st.warning("No faces detected in the photo!")

if __name__ == "__main__":
    main() 